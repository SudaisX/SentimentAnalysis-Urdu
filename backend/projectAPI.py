from __future__ import unicode_literals
# -*- coding: utf-8 -*-
"""AI_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1echUien_ldmT5yrHNwfDrZAysov6yu6B
"""

from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
    
import csv
import pandas as pd
import numpy as np
import numpy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import train_test_split


import string
import codecs
import glob
from collections import Counter
import re
from multiprocessing import Pool
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from pandas import *
import xlrd

def load_file(fileName):
    data = pd.read_excel(fileName)
    return data

def count_words(data):
    Sent = []
    # sentences = sentences.translate(table)
    for sentences in data:
        sentences = str(sentences)
        sentences = re.sub(r"\d+", " ", sentences)
        # English punctuations
        sentences = re.sub(
            r"""[!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~]+""", " ", sentences)
        # Urdu punctuations
        sentences = re.sub(r"[:Ø›ØŸâ€™â€˜Ù­Ø¡ØŒÛ”]+", " ", sentences)
        # Arabic numbers
        sentences = re.sub(r"[Ù â€Ù¡â€Ù¢â€Ù£â€Ù¤â€Ù¥â€Ù¦â€Ù§â€Ù¨â€Ù©]+", " ", sentences)
        sentences = re.sub(r"[^\w\s]", " ", sentences)
        # Remove English characters and numbers.
        sentences = re.sub(r"[a-zA-z0-9]+", " ", sentences)
        # remove multiple spaces.
        sentences = re.sub(r" +", " ", sentences)
        Sent.append(sentences)
        words = sentences.split()
        #print("words: ", words)
        counter = Counter(words)
    #print("Sentences:", Sent)
    vectorizer = CountVectorizer()
    vectorizer.fit(Sent)
    vector = vectorizer.transform(Sent)
    #print("Vocabulary: ", vectorizer.vocabulary_)
    print("2D array:")
    y = vector.toarray()
    print(vector.toarray())
    # print(len(y[0]))
    return vector


def learn_model(data, target):

    classifier = None
    # Your custom implementation of NaiveBayes classifier will go here.
    print("target", target)
    count = -1
    # print(data)
    print("News")
    category = list(np.unique(target))
    for i in category:
        count = count+1
        if i == 3:
            category.pop(count)
    #category = [0, 1, 2]
    print("Category:", category)
    nestedlist = data.toarray()  # coverted to array of vectors
    # print(nestedlist)
    rowcol = nestedlist.shape
    row = rowcol[0]
    col = rowcol[1]
    initialize = np.zeros((len(category), col))
    # since we have 6 categories so each index of each category
    classprob = [0, 0, 0]
    # print(initialize)
    for k in range(len(category)):
        TwoDArray = nestedlist[category[k] == target]
        rowcol2 = TwoDArray.shape
        row2 = rowcol2[0]
        classprob[k] = row2/row
        neww = numpy.sum(TwoDArray, axis=0)
        initialize[k] = neww
        count = 0
        for i in initialize[k]:
            # part b here of Laplacian smoothing to avoid 0 probabilities
            initialize[k][count] = i+1
            count += 1
        neww2 = numpy.sum(initialize[k])
        # this gives us conditional probability of each class.
        initialize[k] = np.divide(initialize[k], neww2)
    # print(classprob)
    # print(initialize)
    final = [classprob, initialize]
    classifier = final
    return classifier


def classify(classifier, testdata):

    predicted_val = []
    # Your code to classify test data using the learned model will go here
    #testingarray = testdata.toarray()
    testingarray = np.array(testdata)
    #print("testing arr", testingarray)
    #x = testingarray
    #print("Each:", len(x[0]))
    classprob = classifier[0]
    conditionalprob = classifier[1]
    rowcol = testingarray.shape
    rows = rowcol[0]
    for i in range(rows):
        problist = []
        for j in range(3):
            mult = 1
            mult = mult*classprob[j]
            for k in range(len(testingarray[i])):
                if testingarray[i][k] >= 1:
                    mult = mult*conditionalprob[j][k]
            problist.append(mult)
        final = max(problist)
        index = problist.index(final)
        if index == 0:
            predicted_val.append(0)
        elif (index == 1):
            predicted_val.append(1)
        else:
            predicted_val.append(2)

    # print(predicted_val)
    return predicted_val


def evaluate(actual_class, predicted_class):

    count = -1
    for i in actual_class:
        count = count+1
        if i == 3:
            actual_class.pop(count)

    print("actual", actual_class)
    accuracy = precision = recall = f_measure = -1
    # print(actual_class)
    # row is predicted and columns are actual
    confusion_matrix = np.zeros((3, 3))
    count = 0
    for j in actual_class:
        if predicted_class[count] == 0:
            row = 0
            count += 1
        elif predicted_class[count] == 1:
            row = 1
            count += 1
        elif predicted_class[count] == 2:
            row = 2
            count += 1
        if j == 0:
            col = 0
        elif j == 1:
            col = 1
        elif j == 2:
            col = 2
        confusion_matrix[row][col] += 1
    divisor = np.sum(confusion_matrix)
    accuracy = np.diag(confusion_matrix).sum()/divisor
    precisionlist = []
    recalllist = []
    f_list = []
    for i in range(confusion_matrix.shape[0]):
        TP = confusion_matrix[i, [i]]
        FP = confusion_matrix[[i], :].sum()-TP
        FN = confusion_matrix[:, [i]].sum()-TP
        TN = confusion_matrix.sum().sum() - TP-FP-FN
        if (TP+FP == 0):
            precisionlist.append(TP+FP)
            recalllist.append(TP+FP)
            f_list.append(TP+FP)
        else:
            precision = TP/(TP+FP)
            precisionlist.append(precision)
            recall = TP/(TP+FN)
            recalllist.append(recall)
            f_measure = (2*TP)/((2*TP)+FN+FP)
            f_list.append(f_measure)

    print("Accuracy:", accuracy)
    print("          Positive ", "       Negative ", "      Neutral ")
    print("Precision:", precisionlist)
    print("Recall:   ", recalllist)
    print("F-measure:", f_list)
    print("Confusion Matrix: \n", confusion_matrix)
    # Write your code to print confusion matrix here
    # references: https://www.youtube.com/watch?v=zqJLLaDd4QQ


def updated_data(data, target, text, count):
    for i in text:
        data.append(i)
    Sent = []
    for sentences in data:
        sentences = str(sentences)
        sentences = re.sub(r"\d+", " ", sentences)
        sentences = re.sub(
            r"""[!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~]+""", " ", sentences)
        sentences = re.sub(r"[:Ø›ØŸâ€™â€˜Ù­Ø¡ØŒÛ”]+", " ", sentences)
        sentences = re.sub(r"[Ù â€Ù¡â€Ù¢â€Ù£â€Ù¤â€Ù¥â€Ù¦â€Ù§â€Ù¨â€Ù©]+", " ", sentences)
        sentences = re.sub(r"[^\w\s]", " ", sentences)
        sentences = re.sub(r"[a-zA-z0-9]+", " ", sentences)
        sentences = re.sub(r" +", " ", sentences)
        Sent.append(sentences)
        words = sentences.split()
        counter = Counter(words)
    vectorizer = CountVectorizer()
    vectorizer.fit(Sent)
    vector = vectorizer.transform(Sent)

    d_arr = vector.toarray()
    totaldata = len(d_arr)
    stop_at = totaldata-count
    testX = []
    for i in range(totaldata, stop_at, -1):
        testX.append(d_arr[i-1])
        target.append(3)
    # print(vector.toarray())
    return testX, data, target




### INPUT HANDLING DONE HERE ##
#text = ["Ø³Ø§Ø±ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ø³ÛŒØ§Ø³Øª Ø¯Ø§Ù† Ù†Ø§Ø§ÛÙ„ Ø³Ú©ØªØ§ ÛÛ’ØŸ Ø¹Ø¯Ù„ÛŒÛ Ù…Ù‚Ù†Ù†Û Ù¾Ø§Ø±Ù„ÛŒÙ…Ù†Ù¹ Ø´Ø§ÛØ¯Ù…Ø³Ø¹ÙˆØ¯", "Ø§Ù„Ù„Û Ù¾Ø§Ú© Ú¾Ù…ÛŒØ´Û Ø³Ù„Ø§Ù…Øª Ø±Ú©Ú¾Û’â€¦", "Ú¯ÙˆØ¬Ø±Û’ Ù†ÛÛŒÚº ØŸ"]
text = ["Ø¨Ù„Ú©Ù„ Ù¹Ú¾ÛŒÚ© Ú©ÛŒØ§ ğŸ‘ Ú©ÛŒÙˆÙ†Ú©Û Ø§Ú¯Ø± Ù…Ø³ØªÙ‚Ù„ ÛÙˆ Ø¬Ø§ØªÛ’ ØªÙˆ Ø¯Ùˆ Ø³Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù†Ú¾ÛŒ Ù…Ù„Ø§Ø²Ù…ÙˆÚº Ù†Û’ Ø¯Ú¾Ø±Ù†Û Ø¯Û’ Ú©Û’ Ø¨ÛŒÙ¹Ú¾ Ø¬Ø§Ù†Ø§ ØªÚ¾Ø§ Ú©Û Ø§Ø¨ ÛÙ…Ø§Ø±Û’ Ù…Ø·Ø§Ù„Ø¨Û’ Ù¾ÙˆØ±Û’ Ú©Ø±Ùˆ ÙˆØ±Ù†Û Ú©Ø§Ù… Ù†ÛÛŒÚº Ú©Ø±ÛŒÚº Ú¯Û’", "Ø¨Ù„Ú©Ù„ Ù¹Ú¾ÛŒÚ© Ú©ÛŒØ§ ğŸ‘ Ú©ÛŒÙˆÙ†Ú©Û Ø§Ú¯Ø± Ù…Ø³ØªÙ‚Ù„ ÛÙˆ Ø¬Ø§ØªÛ’ ØªÙˆ Ø¯Ùˆ Ø³Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù†Ú¾ÛŒ Ù…Ù„Ø§Ø²Ù…ÙˆÚº Ù†Û’ Ø¯Ú¾Ø±Ù†Û Ø¯Û’ Ú©Û’ Ø¨ÛŒÙ¹Ú¾ Ø¬Ø§Ù†Ø§ ØªÚ¾Ø§ Ú©Û Ø§Ø¨ ÛÙ…Ø§Ø±Û’ Ù…Ø·Ø§Ù„Ø¨Û’ Ù¾ÙˆØ±Û’ Ú©Ø±Ùˆ ÙˆØ±Ù†Û Ú©Ø§Ù… Ù†ÛÛŒÚº Ú©Ø±ÛŒÚº Ú¯Û’", "Ø¨Ù„Ú©Ù„ Ù¹Ú¾ÛŒÚ© Ú©ÛŒØ§ ğŸ‘ Ú©ÛŒÙˆÙ†Ú©Û Ø§Ú¯Ø± Ù…Ø³ØªÙ‚Ù„ ÛÙˆ Ø¬Ø§ØªÛ’ ØªÙˆ Ø¯Ùˆ Ø³Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù†Ú¾ÛŒ Ù…Ù„Ø§Ø²Ù…ÙˆÚº Ù†Û’ Ø¯Ú¾Ø±Ù†Û Ø¯Û’ Ú©Û’ Ø¨ÛŒÙ¹Ú¾ Ø¬Ø§Ù†Ø§ ØªÚ¾Ø§ Ú©Û Ø§Ø¨ ÛÙ…Ø§Ø±Û’ Ù…Ø·Ø§Ù„Ø¨Û’ Ù¾ÙˆØ±Û’ Ú©Ø±Ùˆ ÙˆØ±Ù†Û Ú©Ø§Ù… Ù†ÛÛŒÚº Ú©Ø±ÛŒÚº Ú¯Û’"]


class Sentences(BaseModel):
    sentences: list
    
app = FastAPI()

origins = [
    "http://localhost.tiangolo.com",
    "https://localhost.tiangolo.com",
    "http://localhost",
    "http://localhost:8080",
    "http://localhost:3000"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/analyse/")
async def create_item(sentences: Sentences):
    sentence_dict = sentences.dict()
    text = sentence_dict["sentences"]
    # Loading data.....
    dataset = load_file("project.xlsx")
    data, target = dataset['Text'].tolist(), dataset['Sentiment'].tolist()

    count = len(text)
    yourtest, data, target = updated_data(data, target, text, count)

    # Preprocessing data to construct word_vectors.....
    #print("data", data)
    word_vectors = count_words(data)

    trainingX, testX, trainingY, testY = train_test_split(
        word_vectors, target, test_size=0.4, random_state=43)

        # Learning model.....
    model = learn_model(trainingX, trainingY)

    # Classifying test data......
    predictedY = classify(model, yourtest)
    #predictedY = classify(model, yourtest)

    #### PREDICTED VALUES OF YOUR INPUT #####
    predictedY.reverse()
    outputData = []
    for i in range(len(predictedY)):
        outputData.append({
            "sentence": text[i],
            "sentiment": predictedY[i]
        })

    print(outputData)

    return outputData
